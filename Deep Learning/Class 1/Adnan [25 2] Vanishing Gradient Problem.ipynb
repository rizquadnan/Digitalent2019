{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Adnan [25 2] Vanishing Gradient Problem.ipynb","version":"0.3.2","provenance":[{"file_id":"13heRVqan_bxGOzVgRuuVKeniM7L-Llxp","timestamp":1564708578277}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EfwbLGN0zdro","colab_type":"text"},"source":["<img src = \"https://i.imgur.com/UjutVJd.jpg\" align = \"center\">"]},{"cell_type":"markdown","metadata":{"id":"F9MSUizgpeyK","colab_type":"text"},"source":["## Vanishing Gradient Problem"]},{"cell_type":"code","metadata":{"id":"rmb3w0Si0G5b","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bUZrJO7kphMs","colab_type":"text"},"source":["Seperti yang telah dijelaskan, standar Jaringan Saraf Tiruan mengalami beberapa permasalahan yang menyebabkan Jaringan tidak bisa diperdalam. Salah satunya adalah permasalahan *Vanishing Gradient Problem*,\n","\n","Untuk mendalami permasalahan ini, mula-mula mari kita buat kembali implementasi API Neural Net seperti sebelumnya"]},{"cell_type":"code","metadata":{"id":"ipCVw1Anc31h","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b):   \n","  \n","    v = np.dot(x, W) + b # x dot w + b\n","    \n","    cache = (x, W, b)\n","    \n","    return v, cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5-rnF-qc3y3","colab_type":"code","colab":{}},"source":["def affine_backward(dout, cache):\n","    \n","    x, W, b = cache\n","    \n","    dW = np.dot(x.T, dout) # x' dot dout\n","    \n","    db = np.sum(dout, axis=0, keepdims=True)\n","    \n","    dx = np.dot(dout, W.T) # dout dot W'\n","    \n","    return dW, db, dx"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0GrKHRJc3v9","colab_type":"code","colab":{}},"source":["def sigmoid_forward(x):  \n","  \n","    out = 1 / ( 1 + np.exp(-x) ) \n","    \n","    return out  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vv6sCMLRbzrd","colab_type":"code","colab":{}},"source":["def sigmoid_backward(dout, ds):\n","    \"\"\"\n","    Argument:\n","        ds: sigmoid forward result\n","        dout: gradient error\n","    \"\"\"\n","    ds_ = ds * ( 1 - ds )\n","    \n","    dout = dout * ds_\n","    \n","    return dout"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lPVCdwdZqmCg","colab_type":"text"},"source":["### Vanishing Gradient di Fungsi Aktivasi Sigmoid"]},{"cell_type":"markdown","metadata":{"id":"agWfzSMIqKgI","colab_type":"text"},"source":["Fungsi Aktivasi Sigmoid dan Tanh adalah fungsi aktivasi yang hampir selalu digunakan pada awal perkembangan Jaringan Saraf dikarenakan memiliki intepretasi dan intusisi yang pas mengenai *firing rate* dari neuron. Sigmoid menekan output ke dalam rentang $[0..1]$ (dan rentang $[-1..1]$ untuk tanh). \n","\n","\n","*Sigmoid function* | *Tanh function*\n","-- | --\n","![sigmoid](http://cs231n.github.io/assets/nn1/sigmoid.jpeg) | ![tanh](http://cs231n.github.io/assets/nn1/tanh.jpeg)\n","\n","\n","Proses tersebut menyebabkan makin tinggi input sigmoid maka makin mendekati nilai 1 outputnya dengan presisi desimal yang sangat tinggi. Dan hal ini akan menyebabkan nilai gradien yang dikembalikan juga semakin kecil.\n","\n","Perhatikan contoh di bawah"]},{"cell_type":"code","metadata":{"id":"4jEdzjUybzod","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"28fe8297-0352-4657-90e6-baea110486af","executionInfo":{"status":"ok","timestamp":1564717858138,"user_tz":-420,"elapsed":1106,"user":{"displayName":"Muhammad Adnan","photoUrl":"https://lh5.googleusercontent.com/-x59cso3slZA/AAAAAAAAAAI/AAAAAAAAAAc/SK2WAOSPlbQ/s64/photo.jpg","userId":"10968586867277709725"}}},"source":["print('sigmoid(.01) =',sigmoid_forward(.01))\n","print('sigmoid(.1)  =',sigmoid_forward(.1))\n","print('sigmoid(.5)  =',sigmoid_forward(.5))\n","print('sigmoid(1)   =',sigmoid_forward(1))\n","print('sigmoid(10)  =',sigmoid_forward(10))\n","print('sigmoid(50)  =',sigmoid_forward(50))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["sigmoid(.01) = 0.5024999791668749\n","sigmoid(.1)  = 0.52497918747894\n","sigmoid(.5)  = 0.6224593312018546\n","sigmoid(1)   = 0.7310585786300049\n","sigmoid(10)  = 0.9999546021312976\n","sigmoid(50)  = 1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nTtLvRbwbzif","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"38a0af7e-140f-4cc1-81ea-8e9f7111af68","executionInfo":{"status":"ok","timestamp":1564717868129,"user_tz":-420,"elapsed":1153,"user":{"displayName":"Muhammad Adnan","photoUrl":"https://lh5.googleusercontent.com/-x59cso3slZA/AAAAAAAAAAI/AAAAAAAAAAc/SK2WAOSPlbQ/s64/photo.jpg","userId":"10968586867277709725"}}},"source":["print('sigmoid backward(50)  =',sigmoid_backward(1,sigmoid_forward(50)))\n","print('sigmoid backward(10)  =',sigmoid_backward(1,sigmoid_forward(10)))\n","print('sigmoid backward(1)   =',sigmoid_backward(1,sigmoid_forward(1)))\n","print('sigmoid backward(0.5) =',sigmoid_backward(1,sigmoid_forward(.5)))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["sigmoid backward(50)  = 0.0\n","sigmoid backward(10)  = 4.5395807735907655e-05\n","sigmoid backward(1)   = 0.19661193324148185\n","sigmoid backward(0.5) = 0.2350037122015945\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L1Zdhpt8qWnX","colab_type":"text"},"source":["Perhatikan bahwa makin kecil input, makin kecil gradiennya\n","\n","Dan hal ini makin memburuk jika kita memiliki banyak layer. Dikarenakan jika kita memiliki matrik gradien yang berisi nilai desimal, dan saat propagasi balik kita kalikan gradien dengan gradien layer yang juga berisi nilai desimal, maka perkalian tersebut tentunya akan menghasilkan nilai desimal yang lebih kecil lagi. \n","\n","<img src=\"https://image.ibb.co/jSc4ve/gradien.jpg\" alt=\"gradien\"/>\n","\n","Jika nilai tersebut kita bawa hingga ke layer pertama, maka sisa nilai gradien di ujung layer akan limit ke nilai 0, atau bahkan karena keterbatasan bit penyimpanan di komputer menyebabkan gradien menjadi 0. Padahal gradien adalah nilai yang harus ditambahkan ke matrik bobot agar Jaringan belajar\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hu6qEGDuqq7n"},"source":["### Vanishing Gradient di Jaringan Saraf Tiruan"]},{"cell_type":"markdown","metadata":{"id":"3KdlfXtIqt4b","colab_type":"text"},"source":["Sekarang, mari kita coba lihat apa yang terjadi pada gradien jika kita lakukan pelatihan pada suatu Jaringan Saraf Tiruan dengan 10 Layer\n","\n","Mula-mula kita bangkitkan sebuah data sebanyak 1000 data dengan jumlah atribut 500"]},{"cell_type":"code","metadata":{"id":"dqRUez_adNTn","colab_type":"code","colab":{}},"source":["from sklearn.datasets import make_classification\n","from sklearn.preprocessing import minmax_scale\n","\n","x, y = make_classification(n_samples=1000, n_features=500)\n","x = minmax_scale(x, feature_range=(-1, 1))\n","\n","y = np.expand_dims(y, 1)\n","nfitur = x.shape[1]\n","nlabel = y.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ub56GZAOq6jM","colab_type":"text"},"source":["inisialisasi 10 layer jaringan saraf tiruan dengan 512 neuron di setiap layernya"]},{"cell_type":"code","metadata":{"id":"YgM8t-zUdNQz","colab_type":"code","colab":{}},"source":["nhidden = 512\n","\n","w0 = np.random.randn(nfitur , nhidden)*.01 \n","w1 = np.random.randn(nhidden, nhidden)*.01\n","w2 = np.random.randn(nhidden, nhidden)*.01\n","w3 = np.random.randn(nhidden, nhidden)*.01\n","w4 = np.random.randn(nhidden, nhidden)*.01\n","w5 = np.random.randn(nhidden, nhidden)*.01\n","w6 = np.random.randn(nhidden, nhidden)*.01\n","w7 = np.random.randn(nhidden, nhidden)*.01\n","w8 = np.random.randn(nhidden, nlabel )*.01\n","b0 = np.zeros((1, nhidden))\n","b1 = np.zeros((1, nhidden))\n","b2 = np.zeros((1, nhidden))\n","b3 = np.zeros((1, nhidden))\n","b4 = np.zeros((1, nhidden))\n","b5 = np.zeros((1, nhidden))\n","b6 = np.zeros((1, nhidden))\n","b7 = np.zeros((1, nhidden))\n","b8 = np.zeros((1, nlabel))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKWn8nmirAJ5","colab_type":"text"},"source":["#### Forward Pass\n","Mari kita coba propagasi maju pada jaringan dan lihat rata-rata aktivasi di setiap layernya"]},{"cell_type":"code","metadata":{"id":"w4g744u7dNNs","colab_type":"code","colab":{}},"source":["# proses maju\n","layer1, cache1 = affine_forward(x, w0, b0)\n","aktivasi1 = sigmoid_forward(layer1)\n","\n","layer2, cache2 = affine_forward(aktivasi1, w1, b1)\n","aktivasi2 = sigmoid_forward(layer2)\n","\n","layer3, cache3 = affine_forward(aktivasi2, w2, b2)\n","aktivasi3 = sigmoid_forward(layer3)\n","\n","layer4, cache4 = affine_forward(aktivasi3, w3, b3)\n","aktivasi4 = sigmoid_forward(layer4)\n","\n","layer5, cache5 = affine_forward(aktivasi4, w4, b4)\n","aktivasi5 = sigmoid_forward(layer5)\n","\n","layer6, cache6 = affine_forward(aktivasi5, w5, b5)\n","aktivasi6 = sigmoid_forward(layer6)\n","\n","layer7, cache7 = affine_forward(aktivasi6, w6, b6)\n","aktivasi7 = sigmoid_forward(layer7)\n","\n","layer8, cache8 = affine_forward(aktivasi7, w7, b7)\n","aktivasi8 = sigmoid_forward(layer8)\n","\n","layer9, cache9 = affine_forward(aktivasi8, w8, b8)\n","aktivasi9 = sigmoid_forward(layer9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8Ecy6bDd2xP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"4d0fcb72-9fb5-429e-de0b-0595af60cdab","executionInfo":{"status":"ok","timestamp":1564718222541,"user_tz":-420,"elapsed":972,"user":{"displayName":"Muhammad Adnan","photoUrl":"https://lh5.googleusercontent.com/-x59cso3slZA/AAAAAAAAAAI/AAAAAAAAAAc/SK2WAOSPlbQ/s64/photo.jpg","userId":"10968586867277709725"}}},"source":["print('rata-rata aktivasi layer ke-1 = ',np.mean(np.mean(aktivasi1)))\n","print('rata-rata aktivasi layer ke-3 = ',np.mean(np.mean(aktivasi3)))\n","print('rata-rata aktivasi layer ke-5 = ',np.mean(np.mean(aktivasi5)))\n","print('rata-rata aktivasi layer ke-7 = ',np.mean(np.mean(aktivasi7)))\n","print('rata-rata aktivasi layer ke-9 = ',np.mean(np.mean(aktivasi9)))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["rata-rata aktivasi layer ke-1 =  0.5000718445401491\n","rata-rata aktivasi layer ke-3 =  0.4992324863637339\n","rata-rata aktivasi layer ke-5 =  0.5013928842193753\n","rata-rata aktivasi layer ke-7 =  0.5003229023227391\n","rata-rata aktivasi layer ke-9 =  0.44782931105101204\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pu1q_ioQrIpR","colab_type":"text"},"source":["mungkin terlihat rata-rata aktivasi tidak jauh berbeda. Hal ini dikarenakan Jaringan baru mulai dijalankan, sehingga isi bobot setiap layer masih setara"]},{"cell_type":"markdown","metadata":{"id":"kVhOuNeLsmRs","colab_type":"text"},"source":["#### Backward Pass\n","Sekarang, mari kita lihat pada proses mundur untuk menghitung gradiennya"]},{"cell_type":"code","metadata":{"id":"HiKVXLybdNK9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"7afa9f2f-a699-4cd1-8089-e6e2289905d5","executionInfo":{"status":"ok","timestamp":1564718309045,"user_tz":-420,"elapsed":2329,"user":{"displayName":"Muhammad Adnan","photoUrl":"https://lh5.googleusercontent.com/-x59cso3slZA/AAAAAAAAAAI/AAAAAAAAAAc/SK2WAOSPlbQ/s64/photo.jpg","userId":"10968586867277709725"}}},"source":["# hitung error\n","error = y - aktivasi9\n","print(\"mse = %0.20f\" % (np.mean(error ** 2)))\n","\n","\n","#proses mundur\n","g_layer9 = sigmoid_backward(error, aktivasi9)\n","dw8, db8, g_aktivasi8 = affine_backward(g_layer9, cache9)\n","\n","g_layer8 = sigmoid_backward(g_aktivasi8, aktivasi8)\n","dw7, db7, g_aktivasi7 = affine_backward(g_layer8, cache8)\n","\n","g_layer7 = sigmoid_backward(g_aktivasi7, aktivasi7)\n","dw6, db6, g_aktivasi6 = affine_backward(g_layer7, cache7)\n","\n","g_layer6 = sigmoid_backward(g_aktivasi6, aktivasi6)\n","dw5, db5, g_aktivasi5 = affine_backward(g_layer6, cache6)\n","\n","g_layer5 = sigmoid_backward(g_aktivasi5, aktivasi5)\n","dw4, db4, g_aktivasi4 = affine_backward(g_layer5, cache5)\n","\n","g_layer4 = sigmoid_backward(g_aktivasi4, aktivasi4)\n","dw3, db3, g_aktivasi3 = affine_backward(g_layer4, cache4)\n","\n","g_layer3 = sigmoid_backward(g_aktivasi3, aktivasi3)\n","dw2, db2, g_aktivasi2 = affine_backward(g_layer3, cache3)\n","\n","g_layer2 = sigmoid_backward(g_aktivasi2, aktivasi2)\n","dw1, db1, g_aktivasi1 = affine_backward(g_layer2, cache2)\n","\n","g_layer1 = sigmoid_backward(g_aktivasi1, aktivasi1)\n","dw0, db0, dx = affine_backward(g_layer1, cache1)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["mse = 0.2527218\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ICLIYxU6ez-s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"3ea31295-3e27-4649-bde9-7c5f2bc3a4f4","executionInfo":{"status":"ok","timestamp":1564718508648,"user_tz":-420,"elapsed":1154,"user":{"displayName":"Muhammad Adnan","photoUrl":"https://lh5.googleusercontent.com/-x59cso3slZA/AAAAAAAAAAI/AAAAAAAAAAc/SK2WAOSPlbQ/s64/photo.jpg","userId":"10968586867277709725"}}},"source":["print('rata-rata gradien layer ke-9 = ',np.mean(np.mean(g_aktivasi8)))\n","print('rata-rata gradien layer ke-7 = ',np.mean(np.mean(g_aktivasi6)))\n","print('rata-rata gradien layer ke-5 = ',np.mean(np.mean(g_aktivasi4)))\n","print('rata-rata gradien layer ke-3 = ',np.mean(np.mean(g_aktivasi2)))\n","print('rata-rata gradien layer pe-1 = ',np.mean(np.mean(dx)))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["rata-rata gradien layer ke-9 =  -1.0871507056459916e-05\n","rata-rata gradien layer ke-7 =  -2.038765088819387e-08\n","rata-rata gradien layer ke-5 =  7.390546394047515e-11\n","rata-rata gradien layer ke-3 =  1.0433140689723076e-13\n","rata-rata gradien layer pe-1 =  6.324006972723931e-16\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6YUNRKkPsssh","colab_type":"text"},"source":["Dapat dilihat bahwa makin ke depan, rata-rata gradien makin kecil.\n","\n","Ingat, bahwa Gradien adalah nilai yang harus ditambahkan kepada bobot agar bobot belajar.\n","Dan gradien tidak langsung ditambahkan pada bobot, namun hanya seper-bagian kecilnya saja karena akan dikalikan dengan learning rate terlebih dahulu"]},{"cell_type":"markdown","metadata":{"id":"uB_ojCQ6zmKS","colab_type":"text"},"source":["<p>Copyright &copy; 2019 ADF </p>"]}]}